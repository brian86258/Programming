{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "demographic-apparel",
   "metadata": {},
   "source": [
    "# Gradient boosting\n",
    "\n",
    "## What is gradient boosting trees? ‍\n",
    "Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.\n",
    "\n",
    "## What’s the difference between random forest and gradient boosting? ‍\n",
    "* Random Forests builds each tree independently while Gradient Boosting builds one tree at a time. And for Gradient boosting, the process of building tree will also consider the error produced by the previous tree\n",
    "* **Random Forests combine results at the end of the process** (by averaging or \"majority rules\") while **Gradient Boosting combines results along the way.**\n",
    "\n",
    "## Is it possible to parallelize training of a gradient boosting model? How to do it? ‍\n",
    "Since we need the prediction after each tree to update gradients we cannot paralleizing training in the sences of tree level.  However we can achieve **parallelization WITHIN a single tree** my using openMP to create branches independently.\n",
    "\n",
    "## What are the main parameters in the gradient boosting model? ‍\n",
    "\n",
    "There are many parameters, but below are a few key defaults.\n",
    "\n",
    "* learning_rate=0.1 (shrinkage).\n",
    "* n_estimators=100 (number of trees).\n",
    "* max_depth=3.\n",
    "* min_samples_split=2.\n",
    "* min_samples_leaf=1.\n",
    "* subsample=1.0.\n",
    "\n",
    "\n",
    "## How do you approach tuning parameters in XGBoost or LightGBM? \n",
    "\n",
    "Depending upon the dataset, parameter tuning can be done manually or using hyperparameter optimization frameworks such as optuna and hyperopt.   \n",
    "\n",
    "In manual parameter tuning, we need to be aware of **max-depth, min_samples_leaf and min_samples_split so that our model does not overfit** the data but try to predict generalized characteristics of data (basically keeping variance and bias low for our model).\n",
    "\n",
    "## How do you select the number of trees in the gradient boosting model? ‍\n",
    "\n",
    "Most implementations of gradient boosting are configured by default with a relatively small number of trees, such as hundreds or thousands. Using scikit-learn we can perform a grid search of the n_estimators model parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-belize",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
