{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "insured-carroll",
   "metadata": {},
   "source": [
    "# Decision trees\n",
    "## What are the decision trees?\n",
    "This is a type of **supervised learning** algorithm that is applicable for **classification and regression problems(Regression Trees)**. Surprisingly, it works for both **categorical and continuous** (decision trees and regression trees) dependent variables.  \n",
    "In this algorithm, we split the population into two or more homogeneous sets. This is done based on most **significant attributes variables(variables with lowers impurity) to make as distinct groups as possible.**  \n",
    "A decision tree is a tree structure, where each **internal node** (non-leaf node) denotes a **test** on an attribute, each **branch** represents an **outcome of the test**, and each **leaf node** (or terminal node) holds a **value for the target variable**.\n",
    "\n",
    "**Techniques to decide impurity: Gini, Imformation gain, Chi-square, entropy**\n",
    "\n",
    "## How do we train decision trees?\n",
    "1. Start at the root node.\n",
    "2. For each attribute(variable) X, find the condition S that minimize the sum of the node impurities, and **choose the split {X, S}**\n",
    "3, Repeat step to each child node in turn. If stopping criterion is reached(ex,the reamining data is<20...etc ), exit\n",
    "\n",
    "## What are the main parameters of the decision tree model?\n",
    "* maximum tree depth\n",
    "* minimum samples per leaf node\n",
    "* impurity criterion (Ex, Gini, Entropy, Chi-square....)\n",
    "\n",
    "## How do we handle categorical variables in decision trees?\n",
    "Use each possible categorical value to calculate impurity value and further decide wthe condition. \n",
    "We can transform categorical variables, e.g. with a binary or a one-hot encoder.\n",
    "\n",
    "## What are the benefits of a single decision tree compared to more complex models? ‍\n",
    "\n",
    "* easy to implement\n",
    "* fast training\n",
    "* fast inference\n",
    "* good explainability\n",
    "## How can we know which features are more important for the decision tree model? ‍\n",
    "Just because a node is lower on the tree does not necessarily mean that it is less important. The feature importance in sci-kitlearn is calculated by how purely a node separates the classes.  \n",
    "\n",
    "Even if you choose to do the splitting of the nodes at the decision tree according to the Gini Impurity criterion while the importance of the features is given by Gini Importance because Gini Impurity and Gini Importance are not identical \n",
    "\n",
    "## KEY NOTE : It is not necessary that the more important a feature is then the higher its node is at the decision tree.\n",
    "[Explanation](https://datascience.stackexchange.com/questions/16693/interpreting-decision-tree-in-context-of-feature-importances)\n",
    "\n",
    "## Prune Tree(Tree Complexity Penalty)\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "## What is random forest? \n",
    "Random Forest is a machine learning method for **regression and classification** which is **composed of many decision trees.** \n",
    "KEY: Random Forest belongs to a larger class of ML algorithms called **ensemble methods (in other words, it involves the combination of several models to solve a single prediction problem).**\n",
    "\n",
    "## Why do we need randomization in random forest?\n",
    "\n",
    "Randmoization is required in **Bagging algorithm**. Random forest is an extension of bagging algorithm, which takes **random data sample from the training dataset (repetitive sample is allowed) and trains several trees and averages the predictions**.  \n",
    "In addition to that, each time **a split** in a tree is considered, **random forest takes a random sample of m features from full set of n features** (with replacement) and uses this subset of features as candidates for the split (for example, m = sqrt(n)).  \n",
    "\n",
    "Training decision trees on random data samples from the training dataset reduces variance. Sampling features for each split in a decision tree **decorrelates trees.**\n",
    "\n",
    "## What are the main parameters of the random forest model?\n",
    "* max_depth: Longest Path between root node and the leaf\n",
    "* min_sample_split: The minimum number of observations needed to split a given node\n",
    "* max_leaf_nodes: Conditions the splitting of the tree and hence, limits the growth of the trees\n",
    "* min_samples_leaf: minimum number of samples in the leaf node\n",
    "* n_estimators: Number of trees\n",
    "* max_sample: Fraction of original dataset given to any individual tree in the given model\n",
    "* max_features: Limits the maximum number of features provided to trees in random forest model\n",
    "\n",
    "\n",
    "## How do we select the depth of the trees in random forest? ‍\n",
    "\n",
    "The greater the depth, the greater amount of information is extracted from the tree, however, there is a limit to this, and the algorithm even is defensive against overfitting may **learn complex features of noise present in data and as a result,may overfit on noise.**  \n",
    "Hence, there is no hard thumb rule in deciding the depth, but literature suggests a few tips on tuning the depth of the tree to prevent overfitting:\n",
    "\n",
    "    * limit the maximum depth of a tree\n",
    "    * limit the number of test nodes\n",
    "    * limit the minimum number of objects at a node required to split\n",
    "    * do not split a node when, at least, one of the resulting subsample sizes is below a given threshold\n",
    "    * stop developing a node if it does not sufficiently improve the fit.\n",
    "\n",
    "# How do we know how many trees we need in random forest? ‍\n",
    "\n",
    "The number of trees in random forest is worked by n_estimators, and a random forest **reduces overfitting by increasing the number of trees.**  \n",
    "\n",
    "There is **no fixed thumb rule to decide the number of trees** in a random forest, it is rather fine tuned with the data, typically starting off by **taking the square of the number of features (n)** $sqrt(n)$ present in the data followed by tuning until we get the optimal results.\n",
    "\n",
    "\n",
    "# What happens when we have correlated features in our data? ‍\n",
    "In random forest, since random forest samples some features to build each tree, the **information contained in correlated features is twice as much likely to be picked than any other information contained in other features.**  \n",
    "In general, when you are adding correlated features, it means that **they linearly contains the same information and thus it will reduce the robustness of your model.**   \n",
    "\n",
    "Each time you train your model, your model might pick one feature or the other to \"do the same job\" i.e. explain some variance, reduce entropy, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-charleston",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
