{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "changing-september",
   "metadata": {},
   "source": [
    "# Validation\n",
    "\n",
    "## What is overfitting? \n",
    "\n",
    "When your model **perform very well on your training set but can't generalize the test set**, because it adjusted a lot to the training set\n",
    "\n",
    "## How to validate your models?\n",
    "\n",
    "1. One of the most common approaches is splitting data into train, validation and test parts. Models are trained on train data, hyperparameters (for example early stopping) are selected based on the validation data, the final measurement is done on test dataset \n",
    "2. Another approach is cross-validation: split dataset into K folds and each time train models on training folds and measure the performance on the validation folds. Also you could combine these approaches: make a test/holdout dataset and do cross-validation on the rest of the data. The final quality is measured on test dataset.\n",
    "\n",
    "## Why do we need to split our data in to three parts: train, validatoin, test?\n",
    "The **training set is used to fit the model**, i.e. to train the model with the data.Then, the **validation set is then used to provide an unbiased evaluation of a model** while fine-tuning hyperparameters. This improves the generalization of model. Finally, test data set which the model has never \"seen\" before should be used for the final evaluation of the model. This allows for an unbiased evaluation of the model.   \n",
    "The evaluation should never be performed on the same data that is used for training. Otherwise the model performance would not be representative.\n",
    "\n",
    "## Can you explain how cross-validation works? \n",
    "\n",
    "Cross-validation is the process to separate your total training set into two subsets: training and validation set, and evaluate your model to choose the hyperparameters. But you do this process iteratively, selecting differents training and validation set, in order to reduce the bias that you would have by selecting only one validation set.\n",
    "\n",
    "\n",
    "## What is K-fold cross-validation? \n",
    "\n",
    "K fold cross validation is a method of cross validation where we select a hyperparameter k. The dataset is now divided into k parts. Now, we take the 1st part as validation set and remaining k-1 as training set. Then we take the 2nd part as validation set and remaining k-1 parts as training set. Like this, each part is used as validation set once and the remaining k-1 parts are taken together and used as training set. It should not be used in a time series data.\n",
    "\n",
    "## How do we choose K in K-fold cross-validation? Whatâ€™s your favorite K? ðŸ‘¶\n",
    "\n",
    "There are two things to consider while deciding K:  \n",
    "1. **the number of models we get and the size of validation set**. We do not want the number of models to be too less, like 2 or 3. At least 4 models give a less biased decision on the metrics. \n",
    "2. On the other hand, we would want the test dataset to be at least 20-25% of the entire data. So that at least a ratio of 3:1 between training and validation set is maintained.\n",
    "I tend to use 4 for small datasets and 5 for large ones as K.\n",
    "\n",
    "\n",
    "----\n",
    "## Supporting material:\n",
    "[Difference between Model Parameter and Model hyperparameter](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/)  \n",
    "[Fine-tuning](https://stats.stackexchange.com/questions/331369/what-is-meant-by-fine-tuning-of-neural-network)  \n",
    "[Fine Tuning vs Joint Training vs Feature Extraction\n",
    "](https://stats.stackexchange.com/questions/255364/fine-tuning-vs-joint-training-vs-feature-extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-subcommittee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
